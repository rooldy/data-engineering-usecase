# data-engineering-usecase

# ğŸ—ï¸ E-Commerce Data Engineering Pipeline  
Pipeline complet de traitement des transactions dâ€™un site e-commerce avec ingestion, transformation, stockage analytique et orchestration.

---

## ğŸ“Œ 1. Objectif du projet

Ce projet simule un cas rÃ©el dâ€™entreprise :  
Une plateforme e-commerce souhaite suivre ses KPIs clÃ©s :

- CA journalier / hebdomadaire / mensuel  
- Nombre de transactions par catÃ©gorie produit  
- Taux de retour  
- Valeur moyenne du panier (AOV)  
- Top produits vendus  
- ActivitÃ© par client / gÃ©ographie  

Le but est de construire un **pipeline de donnÃ©es de bout en bout**, automatisÃ© et industrialisÃ©.

---

## ğŸ“¦ 2. Architecture du pipeline

Le pipeline repose sur trois couches principales :

### **1. Ingestion**
- Chargement de fichiers CSV/JSON dans une zone *raw*
- Nettoyage minimal
- Gestion du format et horodatage

### **2. Processing (PySpark)**
- Nettoyage avancÃ©
- DÃ©tection dâ€™anomalies
- Normalisation des colonnes
- Enrichissement des donnÃ©es
- Calcul des KPIs intermÃ©diaires

### **3. Analytics**
- CrÃ©ation des datasets analytiques prÃªts pour reporting/dashboards
- AgrÃ©gations PySpark ou Pandas selon le cas

### **4. Orchestration (Airflow)**
- Un DAG unique qui automatise lâ€™ensemble

---

## ğŸ› ï¸ 3. Technologies utilisÃ©es

| Composant | Technologie |
|----------|-------------|
| Orchestration | Apache Airflow |
| Processing Big Data | PySpark |
| Data Exploration | Pandas / Notebooks |
| Conteneurisation | Docker |
| Tests | Pytest |
| CI/CD | GitHub Actions (Ã  venir) |
| Stockage | Local filesystem (simule Data Lake) |

---

## ğŸ“ 4. Structure du projet

```
src/
â”‚â”€â”€ ingestion/
â”‚â”€â”€ processing/
â”‚â”€â”€ analytics/
dags/
docker/
tests/
configs/
notebooks/
data/
```

---

## ğŸš€ 5. ExÃ©cution du projet (en local)

### 1. Cloner le repo  
```bash
git clone https://github.com/rooldy/data-engineering-usecase.git
cd data-engineering-usecase
```

### 2. Installer les dÃ©pendances  
```bash
pip install -r requirements.txt
```

### 3. Lancer Airflow (Docker)  
```bash
docker-compose up --build
```

Lâ€™interface sera disponible sur :  
â¡ï¸ http://localhost:8080

---

## ğŸ§ª 6. Tests

```bash
pytest tests/
```

---

## ğŸ“Œ 7. TODO (feuille de route du projet)

- [ ] Ajouter lâ€™image dâ€™architecture complÃ¨te  
- [ ] Ã‰crire le DAG Airflow  
- [ ] DÃ©velopper ingestion (CSV â†’ raw)  
- [ ] DÃ©velopper transformations PySpark  
- [ ] DÃ©velopper analytics (KPIs)  
- [ ] Ajouter CI/CD GitHub Actions  
- [ ] Ajouter monitoring basique  
- [ ] Ajouter dashboard (Power BI ou Superset)

---

## ğŸ‘¤ Auteur  
**Rooldy â€” Data Engineer**

