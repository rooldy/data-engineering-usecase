# Ã‰tape 5 â€” Architecture Airflow + Design des DAGs

### ğŸ¯ Objectif de lâ€™Ã©tape
Documenter **lâ€™architecture Airflow**, la logique des **DAGs**, les dÃ©pendances, les opÃ©rateurs utilisÃ©s et les bonnes pratiques dâ€™orchestration.  
Cette section montre ta capacitÃ© Ã  **industrialiser un pipeline de donnÃ©es** avec monitoring, gestion des erreurs et automatisation complÃ¨te.

---

# 1. **Architecture Airflow (Vue globale)**

Le projet repose sur une orchestration complÃ¨te via **Apache Airflow**, permettant :

- lâ€™automatisation de lâ€™ingestion â†’ transformation â†’ modÃ©lisation â†’ chargement  
- la surveillance des jobs  
- lâ€™observabilitÃ© (logs, retries, SLA)  
- le versioning via Git  
- la modularitÃ© par DAGs distincts  

### ğŸ§± **Composants Airflow utilisÃ©s**
- **Scheduler** â†’ planification et envoi des tÃ¢ches au Worker  
- **Webserver** â†’ interface de monitoring  
- **Worker / Celery / Kubernetes Executor** (selon setup)  
- **Metadata DB** â†’ suivi des DAG Runs + Task Instances  
- **Connections & Variables** â†’ gestion des secrets et paramÃ¨tres  

---

# 2. **Structure des DAGs dans le projet**

```
airflow/
 â”œâ”€â”€ dags/
 â”‚    â”œâ”€â”€ dag_ingestion.py
 â”‚    â”œâ”€â”€ dag_cleaning_silver.py
 â”‚    â”œâ”€â”€ dag_modeling_gold.py
 â”‚    â”œâ”€â”€ dag_load_to_snowflake.py
 â”‚    â”œâ”€â”€ dag_quality_checks.py
 â”‚    â””â”€â”€ dag_master_pipeline.py
 â”œâ”€â”€ plugins/
 â”‚    â”œâ”€â”€ operators/
 â”‚    â””â”€â”€ hooks/
 â””â”€â”€ configs/
```

---

# 3. **DAG 1 â€” Ingestion (Bronze)**

### ğŸ“Œ Nom : `dag_ingestion.py`

### ğŸ¯ RÃ´le  
Ingestion automatique des donnÃ©es provenant de :
- PostgreSQL  
- API externes  
- IoT / Kinesis  
- Fichiers CSV/JSON sur S3  

### ğŸ§© Tasks typiques  
- `extract_postgres`
- `extract_api_data`
- `extract_iot_stream`
- `upload_to_bronze`

### ğŸ”§ OpÃ©rateurs utilisÃ©s  
- `PythonOperator`  
- `S3UploadOperator`  
- `PostgresHook`  
- `SimpleHttpOperator`

### ğŸ”— DÃ©pendances

```
extract_postgres â†’ upload_to_bronze
extract_api_data â†’ upload_to_bronze
extract_iot_stream â†’ upload_to_bronze
```

---

# 4. **DAG 2 â€” Nettoyage & Normalisation (Silver)**

### ğŸ“Œ Nom : `dag_cleaning_silver.py`

### ğŸ¯ RÃ´le  
Normalisation + qualitÃ© + validation.

### âœ¨ Exemples  
- Conversion dates  
- Correction typages  
- Suppression doublons  
- Standardisation schÃ©mas  

### ğŸ”§ OpÃ©rateurs/techno  
- `GlueJobOperator`  
- `SparkSubmitOperator`  
- `PythonOperator`

### ğŸ”— Flux dâ€™exÃ©cution

```
read_bronze_data
    â†’ apply_cleaning_rules
    â†’ validate_schema
    â†’ write_silver_data
```

---

# 5. **DAG 3 â€” ModÃ©lisation (Gold)**

### ğŸ“Œ Nom : `dag_modeling_gold.py`

### ğŸ¯ RÃ´le  
CrÃ©er les tables mÃ©tier : fact & dimension.

### ğŸ§© Tasks  
- `build_dim_tables`
- `build_fact_ventes`
- `build_fact_energie`
- `merge_gold_tables`

### ğŸ”§ OpÃ©rateurs  
- `SparkSubmitOperator`
- `PythonOperator`

### ğŸ”— Flux

```
build_dim_tables â†’ build_fact_tables â†’ merge_gold_tables
```

---

# 6. **DAG 4 â€” Chargement Snowflake**

### ğŸ“Œ Nom : `dag_load_to_snowflake.py`

### ğŸ¯ RÃ´le  
Charger les donnÃ©es Gold dans Snowflake (ou autre DWH).

### ğŸ“Œ Tasks  
- `stage_upload_files`
- `run_copy_into_commands`
- `refresh_materialized_views`

### ğŸ”§ OpÃ©rateurs  
- `SnowflakeOperator`
- `S3ToSnowflakeOperator`  
- `PythonOperator`  

---

# 7. **DAG 5 â€” ContrÃ´les QualitÃ©**

### ğŸ“Œ Nom : `dag_quality_checks.py`

### ğŸ¯ RÃ´le  
Assurer la qualitÃ© des donnÃ©es via automatisation.

### ğŸ§© Types de tests  
- Tests schÃ©mas  
- UnicitÃ© clÃ© primaire  
- Valeurs nulles  
- Contraintes mÃ©tier  
- Distribution (profiling)

### ğŸ”§ Outils  
- Great Expectations (intÃ©grÃ© Ã  Airflow)
- PythonOperator

---

# 8. **DAG 6 â€” Master Pipeline**

### ğŸ“Œ Nom : `dag_master_pipeline.py`

### ğŸ¯ RÃ´le  
Coordonner lâ€™ensemble des DAGs pour exÃ©cuter un pipeline complet.

### ğŸ”— Structure du DAG global

```
dag_ingestion
      â†“
dag_cleaning_silver
      â†“
dag_modeling_gold
      â†“
dag_quality_checks
      â†“
dag_load_to_snowflake
```

Avec gestion stricte :
- des dÃ©pendances
- des rÃ¨gles de retry
- des SLA par couche

---

# 9. **Gestion des erreurs, alertes & monitoring**

### ğŸš¨ Alerting
- Email
- Slack Webhook
- Push monitoring (Grafana, Prometheus)

### ğŸ” Retry policies
- retry = 3  
- backoff = True  
- retry_delay = 3 minutes

### ğŸ•µï¸ Logs
- Logs stockÃ©s dans S3 / CloudWatch / Loki  
- Toutes les exÃ©cutions sont historisÃ©es

---

# 10. **Bonnes pratiques Airflow appliquÃ©es**

âœ” Un DAG = une logique mÃ©tier  
âœ” Un DAG ne contient pas la logique de transformation (externalisation PySpark)  
âœ” TÃ¢ches idempotentes  
âœ” DÃ©pendances explicites  
âœ” Configuration via Variables Airflow  
âœ” Secrets stockÃ©s dans AWS Secrets Manager  
âœ” Tests unitaires pour les opÃ©rateurs personnalisÃ©s  
âœ” Versioning Git obligatoire avant dÃ©ploiement  

---

# ğŸš€ RÃ©sultat attendu

Une orchestration **hautement professionnelle**, montrant :
- une sÃ©paration claire des responsabilitÃ©s  
- une industrialisation complÃ¨te  
- une observabilitÃ© parfaite  
- une architecture Airflow robuste et scalable  

Cette Ã©tape montre ta capacitÃ© Ã  piloter un pipeline **end-to-end**, digne dâ€™un Data Engineer confirmÃ©.

